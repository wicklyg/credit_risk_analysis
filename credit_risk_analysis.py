# -*- coding: utf-8 -*-
"""Credit_Risk_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1swtNqIoH3rnxF3BLFz58tQV85ZFzNuFt

# Importing Libraries
"""

from google.colab import drive
drive.mount('/content/drive')

# basic
import pandas as pd
import numpy as np

# viz
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import LinearSegmentedColormap
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

plt.style.use('fivethirtyeight')
sns.set_style('white')
sns.set_context('notebook', font_scale=1.5, rc={'lines.linewidth':1.5})
pd.set_option('display.max_colwidth', None)

# encoding
from sklearn.preprocessing import OneHotEncoder

# scaling
from sklearn.preprocessing import StandardScaler

# oversampling
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

# train test split
from sklearn.model_selection import train_test_split

# model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

"""# Importing Dataset"""

path = "/content/drive/MyDrive/ID X Partners Data Scientist/loan_data_2007_2014.csv"

data = pd.read_csv(path)
data.head()

"""# Defining Label / Target Variable"""

data['loan_status'].value_counts(normalize=True)*100

def label_borrower(row):
    if row['loan_status'] == 'Fully Paid':
        return 'Good'
    elif row['loan_status'] == 'Current':
        return 'Good'
    else:
        return 'Bad'

data['label'] = data.apply(label_borrower, axis=1)

pie_data = data['label'].value_counts(normalize=True).values * 100
pie_label = data['label'].value_counts(normalize=True).index

fig, ax = plt.subplots(figsize=(8,6))
wedges, texts, autotexts = ax.pie(pie_data,
                                  startangle=90, explode=[0, 0.1],
                                  autopct='%.2f%%', textprops={'color':'w', 'fontsize':16, 'weight':'bold'})

ax.legend(wedges, pie_label,
          title='Label Borrowers',
          loc='center left', bbox_to_anchor=(1, 0.5))


plt.title("Perbandingan kelas Target")
plt.tight_layout()
plt.show()

"""# Data Pre-processing, Cleaning, and Feature Engineering

* Periksa data (missing value, etc)

Hapus kolom-kolom berikut:

* Hapus kolom 'Unnamed: 0' yang merupakan salinan dari indeks.
* Hapus kolom yang memiliki lebih dari 50% nilai yang hilang. (kolom dengan 0 nilai unik juga merupakan kolom yang memiliki 100% nilai yang hilang)
* Hapus kolom 'application_type' dan 'policy_code' (keduanya hanya memiliki 1 nilai unik).
* Hapus kolom-kolom identitas: id, member_id, title, emp_title, url, zip_code, desc, policy_code (tidak dapat digunakan dalam membangun model).
* Hapus kolom sub_grade, karena berisi informasi yang sama dengan kolom grade.
"""

data.isna().mean().sort_values(ascending=False).head(25)

data.nunique()[data.nunique() < 2].sort_values()

data.drop(['Unnamed: 0', 'desc', 'mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog',
                'annual_inc_joint', 'dti_joint', 'verification_status_joint', 'open_acc_6m', 'open_il_6m',
                'open_il_12m', 'open_il_24m','mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m',
                'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl', 'inq_last_12m', 'policy_code',
                'application_type','id', 'member_id', 'sub_grade', 'emp_title', 'url', 'title',
                'zip_code'], axis=1, inplace = True)

"""# Data Understanding

## Data Leakage

Pemahaman data/kolom sangat penting. Misalnya Kita ingin memprediksi apakah suatu pinjaman berisiko atau tidak, sebelum kami berinvestasi pada pinjaman tersebut, bukan setelahnya. Masalah dengan data terletak pada kolom-kolom yang terkait dengan status terkini pinjaman. Kita hanya dapat memperoleh data dari kolom-kolom tersebut setelah pinjaman diterbitkan, dengan kata lain, setelah kita berinvestasi pada pinjaman tersebut.

* Kolom-kolom yang terkait dengan status terkini pinjaman (setelah diterbitkan): `issue_d`, `loan_status`, `pymnt_plan`, `out_prncp`, `out_prncp_inv`, `total_pymnt`, `total_pymnt_in`, `total_rec_prncp`, `total_rec_int`, `total_rec_late_fee`, `recoveries`, `collection_recovery_fee`, `last_pymnt_d`, `last_pymnt_amnt`, `next_pymnt_d`.

Misalnya, `out_prncp` (sisa pokok pinjaman (sisa pokok pinjaman untuk total dana yang dibiayai)), ketika out_prncp adalah 0, itu berarti pinjaman tersebut sudah lunas sepenuhnya, sangat mudah untuk diprediksi hanya berdasarkan satu variabel ini, dan hasilnya akan sangat akurat. Contoh lainnya adalah `recoveries`, recoveries hanya terjadi setelah peminjam tidak mampu membayar pinjaman dan lembaga pemberi pinjaman memulai proses pemulihan pinjaman. Tentu saja, kami tahu bahwa pinjaman tersebut buruk dan berisiko, hanya dari informasi ini saja. Variabel-variabel tersebut dapat memprediksi dengan sangat akurat karena sudah terjadi.

Dalam ilmu data, jenis variabel seperti ini disebut Data Leakage. Data Leakage adalah terciptanya informasi tambahan yang tidak terduga dalam data pelatihan, yang memungkinkan model atau algoritme pembelajaran mesin membuat prediksi yang tidak realistis. Ini adalah data yang tidak akan kami dapatkan saat menggunakan model dalam implementasi. Kami tidak akan mengetahui apakah akan ada biaya pemulihan, atau apakah pokok pinjaman akan menjadi 0 atau tidak sebelum pinjaman berakhir. Kami tidak akan mendapatkan data-data tersebut sebelum berinvestasi pada pinjaman.

Oleh karena itu, kolom-kolom yang mengandung Data Leakage akan dihapus dan hanya akan mempertahankan kolom dengan data yang dapat diperoleh sebelum pinjaman diinvestasikan.
"""

leakage_col = ['issue_d', 'loan_status', 'pymnt_plan', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv',
                'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',
                'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d']

data.drop(columns=leakage_col, axis=1, inplace=True)

"""## Correlation"""

plt.figure(figsize=(24,24))
sns.heatmap(data.corr(), annot=True, annot_kws={'size':14})

"""*Kolom-kolom `loan_amnt`, `funded_amnt`, dan `funded_amnt_inv` memiliki korelasi yang mirip dengan kolom-kolom lainnya. Oleh karena itu, kemungkinan kolom-kolom ini memiliki data yang hampir sama*"""

# Periksa kolom-kolom yang diduga serupa
data[['loan_amnt','funded_amnt','funded_amnt_inv']].describe()

"""*Data ini sangat mirip, kita dapat menghapus 2 dari kolom tersebut*"""

data.drop(columns = ['funded_amnt', 'funded_amnt_inv'], inplace = True)

"""## Missing Values"""

# Periksa missing values
data.isnull().sum()

data_columns=data.columns[data.isnull().any()].tolist()
data[data_columns].isnull().sum()*100/len(data)

"""`tot_coll_amt`, `tot_cur_bal`, `total_rev_hi_lim` memiliki jumlah nilai yang hilang yang sama sebesar 15% dari seluruh data. Oleh karena itu, ketiga kolom tersebut akan diperiksa.

* `tot_coll_amt`: Jumlah total tagihan yang pernah harus dibayar
* `tot_cur_bal`: Saldo total saat ini dari semua akun
* `total_rev_hi_lim`: Batas kredit/limit kredit total yang tersedia
"""

total_cols = ['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']

data[total_cols].head(10)

data[total_cols].sample(10)

"""*Seperti yang ditunjukkan dalam data di atas, kolom ini memiliki lokasi nilai yang hilang yang serupa.*"""

data[total_cols].describe()

data.boxplot(column=['tot_coll_amt'])
plt.show()

data.boxplot(column=['tot_cur_bal'])
plt.show()

data.boxplot(column=['total_rev_hi_lim'])
plt.show()

"""Kesimpulan:

* 75% dari tot_coll_amt bernilai 0.
* Data untuk setiap baris cukup berbeda sehingga tidak mungkin mengisi nilai yang hilang dengan nilai rata-rata atau nilai lainnya.
* Jumlah total nilai yang hilang adalah 70276, yang merupakan 15,07% dari seluruh data.
* Oleh karena itu, baris-baris yang memiliki nilai yang hilang pada kolom-kolom tersebut akan dihapus.
"""

data.dropna(subset = ['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim'], inplace = True)

data.reset_index(drop= True, inplace = True)

"""## Data Type Transformation

Mengubah tipe data yang tidak sesuai.
"""

continuous_cols = ['term', 'emp_length', 'earliest_cr_line', 'last_credit_pull_d']
data[continuous_cols]

"""### 1. term

`term`: Jumlah pembayaran atas pinjaman. Nilai dalam beberapa bulan dan dapat berupa 36 atau 60.
"""

# Ubah ke tipe data numerik
data['term'] = pd.to_numeric(data['term'].str.replace(' months', ''))
data['term']

"""### 2. emp_length

`emp_length`: Lama pekerjaan dalam beberapa tahun. Nilai yang mungkin adalah antara 0 dan 10 di mana 0 berarti kurang dari satu tahun dan 10 berarti sepuluh tahun atau lebih.
"""

data['emp_length'].unique()

emp_map = {
    '< 1 year' : '0',
    '1 year' : '1',
    '2 years' : '2',
    '3 years' : '3',
    '4 years' : '4',
    '5 years' : '5',
    '6 years' : '6',
    '7 years' : '7',
    '8 years' : '8',
    '9 years' : '9',
    '10+ years' : '10'
}

data['emp_length'] = data['emp_length'].map(emp_map).fillna('0').astype(int)
data['emp_length'].unique()

"""### 3. earliest_cr_line

`earliest_cr_line`:Bulan ketika garis kredit tercatat peminjam dibuka
"""

data['earliest_cr_line_date'] = pd.to_datetime(data['earliest_cr_line'], format='%b-%y')

data['earliest_cr_line_date']

# Asumsikan sekarang bulan Desember 2017
data['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['earliest_cr_line_date']) / np.timedelta64(1, 'M')))

data['mths_since_earliest_cr_line'].describe()

data.loc[: , ['earliest_cr_line', 'earliest_cr_line_date',
                   'mths_since_earliest_cr_line']][data['mths_since_earliest_cr_line'] < 0]

"""*Tanggal sebelum tahun 1970 tidak dikonversi dengan baik, menjadi tahun 2069 dan sejenisnya, dan perbedaan negatif dihitung.*"""

data['earliest_cr_line_date'] = data['earliest_cr_line_date'].astype(str)
data['earliest_cr_line_date'][data['mths_since_earliest_cr_line'] < 0] = data['earliest_cr_line_date'][data['mths_since_earliest_cr_line'] < 0].str.replace('20','19')

# ubah type data ke datetime
data['earliest_cr_line_date'] = pd.to_datetime(data['earliest_cr_line_date'])
data['earliest_cr_line_date']

# Asumsikan sekarang bulan Desember 2015
data['mths_since_earliest_cr_line_date'] = round(pd.to_numeric((pd.to_datetime('2015-12-01') - data['earliest_cr_line_date']) / np.timedelta64(1, 'M')))

data['mths_since_earliest_cr_line_date'].describe()

"""*Tidak ada lagi nilai negatif. Dan data telah dikonversi dengan baik.*"""

data.drop(columns = ['earliest_cr_line_date' ,'mths_since_earliest_cr_line',
                    'earliest_cr_line'], inplace = True)

"""### 4. last_credit_pull_d

`last_credit_pull_d`: Bulan di mana pinjaman dibiayai.
"""

# Asumsikan sekarang bulan Desember 2017
# Ekstrak tanggal dan waktu dari variabel string yang memiliki format tertentu, dan isi data NaN dengan tanggal maksimum
data['last_credit_pull_d'] = pd.to_datetime(data['last_credit_pull_d'], format = '%b-%y').fillna(pd.to_datetime("2016-01-01"))

# Menghitung selisih antara dua tanggal dalam bulan, mengubahnya menjadi tipe data numerik, dan membulatkannya.
data['mths_since_last_credit_pull_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['last_credit_pull_d']) / np.timedelta64(1, 'M')))

data['mths_since_last_credit_pull_d'].describe()

"""*Tidak ada lagi nilai negatif. Dan data telah dikonversi dengan baik.*"""

data.drop(columns = ['last_credit_pull_d'], inplace = True)

"""## Periksa missing value lagi"""

data.isnull().sum()

data.dropna(subset = ['revol_util'], inplace = True)

data.reset_index(drop= True, inplace = True)

data.isnull().sum()

"""*Tidak ada missing value*

## Explore Data
"""

def risk_percentage(x):
    ratio = (data.groupby(x)['label'] # group by
         .value_counts(normalize=True) # menghitung rasio
         .mul(100) # mengubah ke persen
         .rename('risky (%)') # rename kolom
         .reset_index())

    sns.lineplot(data=ratio[ratio['label'] == 'Bad'], x=x, y='risky (%)')
    plt.title(x)
    plt.show()

print(data.nunique()[data.nunique() < 12].sort_values().index)

unq_cols = ['term', 'initial_list_status', 'verification_status',
            'home_ownership', 'acc_now_delinq', 'grade', 'inq_last_6mths',
            'collections_12_mths_ex_med', 'emp_length',
            'mths_since_earliest_cr_line_date', 'mths_since_last_credit_pull_d']

for col in unq_cols:
    risk_percentage(col)

"""Kesimpulan:

* `term`: memiliki risiko rendah pada periode 36 bulan dan risiko tinggi pada periode 60 bulan
* `initial_list_status`: memiliki risiko tinggi pada status f dan risiko rendah pada status w
* `verification_status`: memiliki risiko tinggi pada status terverifikasi
* `home_ownership`: memiliki risiko tinggi pada jenis kepemilikan None dan Other.
* `acc_now_delinq`: memiliki risiko rendah dengan nilai 2, 0, 1, dan memiliki risiko tinggi dengan nilai 3, 4, 5
* `grade`: terdapat peningkatan risiko yang terkait dengan ini.
* `inq_last_6mths`: terdapat peningkatan risiko yang terkait dengan ini.
* `collections_12_mths_ex_med`: memiliki risiko rendah dengan nilai 3.0 dan memiliki risiko tinggi dengan nilai 4.0
* `emp_length`:Lama bekerja kurang dari 1 tahun memiliki persentase risiko terbesar dan lama bekerja lebih dari 9 tahun memiliki persentase risiko terkecil
* `mths_since_earliest_cr_line_date`: Semakin awal batas kredit, semakin stabil catatan peminjam, dan terdapat peningkatan risiko yang terkait dengan ini.
* `mths_since_last_credit_pull_d`: memiliki variasi risiko yang berbeda-beda.

## Encoding
"""

label_map = {'Bad': 0, 'Good': 1}

data['label'] = data['label'].map(label_map)
data['label'].unique()

# One Hot Encoding

cat_cols = [col for col in data.select_dtypes(include='object').columns.tolist()]
onehot_cols = pd.get_dummies(data[cat_cols], drop_first=True)

onehot_cols

"""## Standardization"""

num_cols = [col for col in data.columns.tolist() if col not in cat_cols + ['label']]
ss = StandardScaler()
std_cols = pd.DataFrame(ss.fit_transform(data[num_cols]), columns=num_cols)

std_cols

std_cols.describe()

"""## Combining column"""

final_data = pd.concat([onehot_cols, std_cols, data[['label']]], axis=1)
final_data.head()

"""## Split Data"""

# Definisikan X dan y
X = final_data.drop('label', axis = 1)
y = final_data['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    random_state=42,stratify=y)

X_train.shape, X_test.shape

"""# Handling Imbalace data

## Random Oversampling
"""

over = RandomOverSampler(sampling_strategy = 'minority')
X_train_over, y_train_over = over.fit_resample(X_train, y_train)

# periksa jumlah data sebelum dan sesudah oversampling
print('Sebelum OverSampling:\n{}'.format(y_train.value_counts()))
print('\nSesudah OverSampling:\n{}'.format(y_train_over.value_counts()))

"""## Random Undersampling"""

under = RandomUnderSampler(sampling_strategy = 'majority')
X_train_under, y_train_under = under.fit_resample(X_train, y_train)

# periksa jumlah data sebelum dan sesudah undersampling
print('Sebelum UnderSampling:\n{}'.format(y_train.value_counts()))
print('\nSesudah UnderSampling:\n{}'.format(y_train_under.value_counts()))

"""## SMOTE"""

smote = SMOTE()
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# periksa jumlah data sebelum dan sesudah undersampling
print('Sebelum OverSampling:\n{}'.format(y_train.value_counts()))
print('\nSesudah OverSampling:\n{}'.format(y_train_smote.value_counts()))

"""# Modelling

## Random Forest
"""

def train_random_forest(sampling_method='none'):

    model = RandomForestClassifier()

    if sampling_method == 'over':
        # Oversampling with RandomOverSampler
        model.fit(X_train_over, y_train_over)

    elif sampling_method == 'under':
        # Undersampling with RandomUnderSampler
        model.fit(X_train_under, y_train_under)

    elif sampling_method == 'smote':
        # SMOTE
        model.fit(X_train_smote, y_train_smote)

    else:
        model.fit(X_train, y_train)


    y_pred = model.predict(X_test)

    cm = confusion_matrix(y_test, y_pred)

    target_names = ['Bad Borrowers', 'Good Borrowers']

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=target_names, yticklabels=target_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    # Print classification report
    print("Classification Report:")
    print(classification_report(y_test, y_pred, target_names = target_names))

random_forest = train_random_forest()
random_forest

random_forest_over = train_random_forest(sampling_method='over')
random_forest_over

random_forest_under = train_random_forest(sampling_method='under')
random_forest_under

random_forest_smote = train_random_forest(sampling_method='smote')
random_forest_smote



"""## Logistic Regression"""

def train_reglog(sampling_method='none'):

    model = LogisticRegression(max_iter=1000)

    if sampling_method == 'over':
        # Oversampling with RandomOverSampler
        model.fit(X_train_over, y_train_over)

    elif sampling_method == 'under':
        # Undersampling with RandomUnderSampler
        model.fit(X_train_under, y_train_under)

    elif sampling_method == 'smote':
        # SMOTE
        model.fit(X_train_smote, y_train_smote)

    else:
        model.fit(X_train, y_train)


    y_pred = model.predict(X_test)

    cm = confusion_matrix(y_test, y_pred)

    target_names = ['Bad Borrowers', 'Good Borrowers']

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=target_names, yticklabels=target_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    # Print classification report
    print("Classification Report:")
    print(classification_report(y_test, y_pred, target_names = target_names))

reglog = train_reglog()
reglog

reglog_over = train_reglog(sampling_method = 'over')
reglog_over

reglog_under = train_reglog(sampling_method = 'under')
reglog_under

reglog_smote = train_reglog(sampling_method = 'smote')
reglog_smote



"""# Conclusion

*   Model terbaik adalah Random Forest menggunakan metode Random Oversampling dengan akurasi sebesar ~89%
"""

